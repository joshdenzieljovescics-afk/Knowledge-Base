import pandas as pd
import re
from typing import Dict, List, Any, Tuple
import numpy as np
import os
import json
from openai import OpenAI


class SmartMappingEngine:
    """
    Hybrid smart column mapping engine - uses 3-tier approach:
    1. Exact matching (instant, free, perfect)
    2. Rule-based semantic matching (fast, free, good)
    3. OpenAI LLM fallback (slow, paid, excellent for edge cases)
    """

    def __init__(self, use_openai: bool = True):
        # Import the actual SafExpressOps columns
        from safexpressops_target_columns import (
            SAFEXPRESSOPS_TARGET_COLUMNS,
            CALCULATED_COLUMNS,
            INPUT_COLUMNS,
            is_calculated_column,
        )

        # Store the full list
        self.all_target_columns = SAFEXPRESSOPS_TARGET_COLUMNS

        self.calculated_columns = CALCULATED_COLUMNS
        self.input_columns = INPUT_COLUMNS
        self.is_calculated = is_calculated_column

        # OpenAI setup
        self.use_openai = use_openai
        if use_openai and os.getenv("OPENAI_API_KEY"):
            self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            print("âœ… OpenAI integration enabled for smart mapping")
        else:
            self.use_openai = False
            print("âš ï¸ OpenAI integration disabled (no API key or use_openai=False)")

        # SafexpressOps operational terms - business language
        self.operational_vocabulary = {
            # safety domain
            "safety": {
                "keywords": [
                    "safety",
                    "incident",
                    "accident",
                    "injury",
                    "lost",
                    "time",
                    "manhour",
                    "hour",
                ],
                "target_columns": [
                    col
                    for col in SAFEXPRESSOPS_TARGET_COLUMNS
                    if any(
                        kw in col.lower()
                        for kw in ["manhour", "safe", "incident", "lost time"]
                    )
                ],
            },
            # warehouse domain
            "warehouse": {
                "keywords": [
                    "warehouse",
                    "cv",
                    "container",
                    "vessel",
                    "pick",
                    "cycle",
                    "count",
                    "damage",
                    "fefo",
                ],
                "target_columns": [
                    col
                    for col in SAFEXPRESSOPS_TARGET_COLUMNS
                    if any(
                        kw in col.lower()
                        for kw in [
                            "cv received",
                            "picked qty",
                            "cycle count",
                            "warehouse damage",
                            "fefo",
                        ]
                    )
                ],
            },
            # quality domain
            "quality": {
                "keywords": [
                    "quality",
                    "accuracy",
                    "error",
                    "defect",
                    "cts",
                    "customer",
                    "satisfaction",
                ],
                "target_columns": [
                    col
                    for col in SAFEXPRESSOPS_TARGET_COLUMNS
                    if any(
                        kw in col.lower()
                        for kw in ["cts", "accuracy", "expired", "quality"]
                    )
                ],
            },
            # Attendance Domain
            "attendance": {
                "keywords": [
                    "present",
                    "attendance",
                    "staff",
                    "employee",
                    "people",
                    "worker",
                    "deployed",
                    "manpower",
                ],
                "target_columns": [
                    col
                    for col in SAFEXPRESSOPS_TARGET_COLUMNS
                    if any(
                        kw in col.lower()
                        for kw in [
                            "present",
                            "attendance",
                            "deployed",
                            "manpower",
                            "late",
                        ]
                    )
                ],
            },
        }

        # Common operational abbreviations - expand these automatically
        self.abbreviation_expansion = {
            "cv": "container vessel",
            "otif": "on time in full",
            "cts": "customer satisfaction",
            "fefo": "first expired first out",
            "hrs": "hours",
            "hours": "manhours",
            "hr": "hours",
            "qty": "quantity",
            "ave": "average",
            "avg": "average",
            "pct": "percent",
            "acc": "accuracy",
        }

    def smart_map_columns(
        self,
        source_columns: Any,
        target_columns: List[str],
        sample_data: pd.DataFrame = None,
    ) -> Dict[str, str]:
        """
        Main function: 3-tier intelligent mapping

        Args:
            source_columns: Columns from uploaded file
            target_columns: SafExpressOps target columns
            sample_data: Sample of the actual data to analyze

        Returns:
            Dictionary mapping source to target columns with confidence scores
        """

        print("\nðŸ§  Smart Mapping Engine starting...")
        print(f"   Source columns: {len(source_columns)}")
        print(f"   Target columns: {len(target_columns)}")

        original_count = len(target_columns)
        target_columns = [col for col in target_columns if not self.is_calculated(col)]

        # Show what was filtered
        print(f"\nðŸ” Formula-Aware Filtering:")
        print(f"   Calculated excluded: {original_count - len(target_columns)}")
        print(f"   Mappable targets: {len(target_columns)}")

        # Tier 1: Exact matching (instant, free, perfect)
        print("\nðŸŽ¯ Tier 1: Exact matching...")
        exact_matches, remaining_sources = self._exact_matching(
            source_columns, target_columns
        )
        remaining_targets = [
            t for t in target_columns if t not in exact_matches.values()
        ]

        print(f"   âœ… Exact matches: {len(exact_matches)}")
        print(
            f"   Remaining: {len(remaining_sources)} sources, {len(remaining_targets)} targets"
        )

        # Tier 2: Rule-based semantic mapping (fast, free, good)
        semantic_mappings = {}
        if remaining_sources:
            print("\nðŸ“Š Tier 2: Semantic matching...")
            semantic_mappings = self._semantic_mapping(
                remaining_sources, remaining_targets
            )

            # Analyze data patterns if sample data is provided
            if sample_data is not None:
                data_insights = self._analyze_data_patterns(
                    remaining_sources, sample_data
                )
                semantic_mappings = self._apply_data_insights(
                    semantic_mappings, data_insights, remaining_targets
                )

        # Tier 3: OpenAI LLM for difficult cases (slow, paid, excellent)
        openai_mappings = {}
        if self.use_openai and remaining_sources:
            # Only send low-confidence mappings to OpenAI
            low_confidence_sources = self._get_low_confidence_sources(
                semantic_mappings, remaining_sources, threshold=0.6
            )

            if low_confidence_sources:
                print(
                    f"\nðŸ¤– Tier 3: OpenAI LLM for {len(low_confidence_sources)} difficult columns..."
                )
                openai_mappings = self._openai_mapping(
                    low_confidence_sources, remaining_targets, sample_data
                )
                print(f"   âœ… OpenAI returned {len(openai_mappings)} mappings")

        # Combine all tiers
        final_mappings = self._combine_tiers(
            exact_matches,
            semantic_mappings,
            openai_mappings,
            source_columns,
            remaining_sources,
        )

        return final_mappings

    def _exact_matching(
        self, source_cols: List[str], target_cols: List[str]
    ) -> Tuple[Dict[str, str], List[str]]:
        """
        Tier 1: Find exact matches (case-insensitive)
        Returns: (exact_matches_dict, remaining_sources)
        """
        exact_matches = {}
        remaining_sources = []
        remaining_targets = list(target_cols)

        for source_col in source_cols:
            exact_match = None
            for target_col in remaining_targets:
                if source_col.lower().strip() == target_col.lower().strip():
                    exact_match = target_col
                    break

            if exact_match:
                exact_matches[source_col] = exact_match
                remaining_targets.remove(exact_match)
                if len(exact_matches) <= 5:  # Only print first 5
                    print(f"      âœ“ '{source_col}' â†’ '{exact_match}'")
            else:
                remaining_sources.append(source_col)

        if len(exact_matches) > 5:
            print(f"      ... and {len(exact_matches) - 5} more exact matches")

        return exact_matches, remaining_sources

    def _get_low_confidence_sources(
        self, semantic_mappings: Dict, source_cols: List[str], threshold: float = 0.6
    ) -> List[str]:
        """
        Identify columns that need OpenAI help (low confidence from semantic matching)
        """
        low_confidence = []

        for source_col in source_cols:
            if source_col not in semantic_mappings:
                low_confidence.append(source_col)
                continue

            # Find best score for this source
            best_score = (
                max(semantic_mappings[source_col].values())
                if semantic_mappings[source_col]
                else 0
            )

            if best_score < threshold:
                low_confidence.append(source_col)

        return low_confidence

    def _openai_mapping(
        self,
        source_cols: List[str],
        target_cols: List[str],
        sample_data: pd.DataFrame = None,
    ) -> Dict[str, str]:
        """
        Tier 3: Use OpenAI to map difficult columns
        """
        if not self.use_openai:
            return {}

        # Prepare sample data context
        sample_context = ""
        if sample_data is not None and len(source_cols) > 0:
            sample_context = "\n\nSample data (first 3 rows):\n"
            for source_col in source_cols[:10]:  # Show up to 10 columns
                if source_col in sample_data.columns:
                    values = sample_data[source_col].head(3).tolist()
                    sample_context += f"- {source_col}: {values}\n"

        # Create prompt
        prompt = f"""You are a data mapping expert for SafExpressOps, a warehouse operations company.

**Task**: Map source column names to target column names.

**Source columns to map** ({len(source_cols)} columns):
{json.dumps(source_cols, indent=2)}

**Available target columns** ({len(target_cols)} columns):
{json.dumps(target_cols, indent=2)}
{sample_context}

**Business context**:
- SafExpressOps tracks: safety metrics, warehouse operations, quality, inventory, expenses
- Common abbreviations: CV = Container Vessel, FEFO = First Expired First Out, CTS = Customer Satisfaction
- "Manhours" refers to labor hours worked
- "Incidents" are safety/quality issues
- "OTD" = On-Time Delivery, "OTIF" = On-Time In Full

**Instructions**:
1. For each source column, find the BEST matching target column
2. Only map if you're confident (>70% confidence)
3. Return JSON format: {{"source_column": "target_column" or null}}
4. If no good match exists, use null
5. Consider abbreviations and business context

Return ONLY valid JSON, no explanation:"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",  # Fast and cheap
                messages=[
                    {
                        "role": "system",
                        "content": "You are a data mapping expert. Return only valid JSON.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,  # Low temperature for consistent results
                max_tokens=2000,
            )

            # Parse response
            response_text = response.choices[0].message.content.strip()

            # Remove markdown code blocks if present
            if response_text.startswith("```json"):
                response_text = response_text[7:-3].strip()
            elif response_text.startswith("```"):
                response_text = response_text[3:-3].strip()

            mappings = json.loads(response_text)

            # Validate mappings
            validated_mappings = {}
            for source, target in mappings.items():
                if target and target in target_cols:
                    validated_mappings[source] = target
                    print(f"      âœ“ '{source}' â†’ '{target}' (via OpenAI)")

            return validated_mappings

        except Exception as e:
            print(f"      âš ï¸ OpenAI mapping failed: {str(e)}")
            return {}

    def _combine_tiers(
        self,
        exact_matches: Dict[str, str],
        semantic_mappings: Dict,
        openai_mappings: Dict[str, str],
        all_source_cols: List[str],
        remaining_sources: List[str],
    ) -> Dict[str, Any]:
        """
        Combine results from all 3 tiers into final mappings
        """
        # âœ… SIMPLIFIED BLOCKING LIST - Only truly confusing pairs remain
        # With simplified column names, semantic matching now works correctly!
        incorrect_pairs_set = {
            # ============================================================
            # Cross-category confusion (different types of metrics)
            # ============================================================
            ("Warehouse Damage Incident", "Warehouse Damage Incident Cost"),
            ("Space Utilization", "Truck Utilization"),
            ("Truck Utilization", "Space Utilization"),
            # Different incident types
            ("Warehouse Damage Incident", "Losttime Incident"),
            ("FEFO Incident", "Losttime Incident"),
            ("Expired Product Incident", "Losttime Incident"),
            # Inventory vs Expenses
            ("Good Pallet Inventory", "LPG Expenses"),
            ("Good Pallet Inventory", "Diesel Expenses"),
            ("Damaged Pallet Inventory", "Whse Damaged Pallet Cost"),
            ("Total Stock On-Hand", "Total Expenses"),
            # Different expense types
            ("LPG Expenses", "Diesel Expenses"),
            ("Office Supplies", "Meals Expenses"),
            # POD vs Transmission
            ("Returned POD", "Transmitted to Client"),
            ("Unreturned POD", "Not yet Transmitted"),
            ("Return Performance", "Trasmitted Perfromance"),
            # Manpower types
            ("Manpower Matrix", "Deployed"),
            ("Deployed", "Present"),
            # Performance metrics
            ("Manpower Fill-rate", "Attendance Perf %"),
            ("Attendance Perf %", "Timeliness Perf. %"),
        }

        # ============================================================
        # NOTE: With simplified column names, we no longer need:
        # ============================================================
        # âŒ Units Received vs Units Dispatched blocking (names are now distinct)
        # âŒ HIT vs MISS cross-blocking (names are now distinct)
        # âŒ Loaded vs Received blocking (names are now distinct)
        # âŒ Loading vs Unloading blocking (names are now distinct)
        # âŒ Discrepancy Inbound vs Outbound blocking (names are explicit)
        #
        # The new names make semantic matching work correctly!

        final_mappings = {}

        # Add exact matches (highest confidence)
        for source_col, target_col in exact_matches.items():
            # Block wrong pairs
            if (source_col, target_col) in incorrect_pairs_set:
                print(f"   ðŸš« BLOCKED (exact): {source_col} â†’ {target_col}")
                final_mappings[source_col] = {
                    "target": None,
                    "confidence_score": 0.0,
                    "confidence_level": "blocked",
                    "needs_review": True,
                    "method": "blocked",
                }
                continue

            final_mappings[source_col] = {
                "target": target_col,
                "confidence_score": 1.0,
                "confidence_level": "high",
                "needs_review": False,
                "method": "exact_match",
            }

        # Add OpenAI mappings (overrides semantic for low-confidence)
        for source_col, target_col in openai_mappings.items():
            # âœ… ALSO block OpenAI suggestions
            if (source_col, target_col) in incorrect_pairs_set:
                print(f"   ðŸš« BLOCKED (openai): {source_col} â†’ {target_col}")
                final_mappings[source_col] = {
                    "target": None,
                    "confidence_score": 0.0,
                    "confidence_level": "blocked",
                    "needs_review": True,
                    "method": "blocked",
                }
                continue

            final_mappings[source_col] = {
                "target": target_col,
                "confidence_score": 0.85,  # OpenAI gets high confidence
                "confidence_level": "high",
                "needs_review": False,
                "method": "openai_llm",
            }

        # Add semantic mappings for remaining columns
        remaining_targets = [
            t
            for t in self.all_target_columns
            if t not in exact_matches.values() and t not in openai_mappings.values()
        ]

        for source_col in remaining_sources:
            if source_col in openai_mappings:
                continue  # Already handled by OpenAI

            # Find best semantic match
            best_target = None
            best_score = 0.0

            if source_col in semantic_mappings:
                for target_col in remaining_targets:
                    score = semantic_mappings[source_col].get(target_col, 0)
                    if score > best_score:
                        best_score = score
                        best_target = target_col

            # âœ… CRITICAL FIX: Block semantic matches BEFORE accepting them
            if best_target and (source_col, best_target) in incorrect_pairs_set:
                print(f"   ðŸš« BLOCKED (semantic): {source_col} â†’ {best_target}")
                best_target = None
                best_score = 0.0
                confidence_level = "blocked"
            else:
                # Determine confidence level
                if best_score >= 0.7:
                    confidence_level = "high"
                elif best_score >= 0.5:
                    confidence_level = "medium"
                else:
                    confidence_level = "low"
                    best_target = None  # Don't map if confidence too low

            final_mappings[source_col] = {
                "target": best_target,
                "confidence_score": best_score,
                "confidence_level": confidence_level,
                "needs_review": confidence_level in ["low", "medium", "blocked"],
                "method": (
                    "semantic"
                    if best_target
                    else ("blocked" if confidence_level == "blocked" else "none")
                ),
            }

        # Create summary
        high_confidence = sum(
            1 for v in final_mappings.values() if v["confidence_level"] == "high"
        )
        needs_review = sum(1 for v in final_mappings.values() if v["needs_review"])
        blocked_count = sum(
            1 for v in final_mappings.values() if v["confidence_level"] == "blocked"
        )

        print(f"\nðŸ“Š Final Mapping Summary:")
        print(f"   Total columns: {len(all_source_cols)}")
        print(f"   Exact matches: {len(exact_matches)}")
        print(f"   OpenAI matches: {len(openai_mappings)}")
        print(
            f"   Semantic matches: {len([v for v in final_mappings.values() if v.get('method') == 'semantic'])}"
        )
        print(f"   Blocked (wrong pairs): {blocked_count}")
        print(f"   High confidence: {high_confidence}")
        print(f"   Needs review: {needs_review}")
        print(f"   Accuracy: {high_confidence / len(all_source_cols) * 100:.1f}%")

        return {
            "mappings": final_mappings,
            "summary": {
                "total_columns": len(all_source_cols),
                "high_confidence_mappings": high_confidence,
                "needs_review": needs_review,
                "blocked_mappings": blocked_count,
                "accuracy_estimate": (
                    high_confidence / len(all_source_cols) if all_source_cols else 0
                ),
                "methods_used": {
                    "exact": len(exact_matches),
                    "openai": len(openai_mappings),
                    "semantic": len(
                        [
                            v
                            for v in final_mappings.values()
                            if v.get("method") == "semantic"
                        ]
                    ),
                    "blocked": blocked_count,
                },
            },
        }

    # ============================================================
    # EXISTING METHODS (Keep these unchanged)
    # ============================================================

    def _semantic_mapping(
        self, source_cols: List[str], target_cols: List[str]
    ) -> Dict[str, Dict[str, float]]:
        """Step 1: Better than simple string matching - understands meaning"""
        mappings = {}

        for source_col in source_cols:
            mappings[source_col] = {}
            source_clean = self._clean_and_expand(source_col)

            for target_col in target_cols:
                target_clean = self._clean_and_expand(target_col)
                similarity = self._calculate_semantic_similarity(
                    source_clean, target_clean
                )
                mappings[source_col][target_col] = similarity

        return mappings

    def _clean_and_expand(self, column_name: str) -> str:
        """Clean column names and expand abbreviations"""
        cleaned = re.sub(r"[^\w\s]", " ", column_name.lower())
        words = cleaned.split()
        expanded_words = []

        for word in words:
            if word in self.abbreviation_expansion:
                expanded_words.extend(self.abbreviation_expansion[word].split())
            else:
                expanded_words.append(word)

        return " ".join(expanded_words)

    def _calculate_semantic_similarity(
        self, source_clean: str, target_clean: str
    ) -> float:
        """Calculate how similar two column names are semantically"""
        source_words = set(source_clean.split())
        target_words = set(target_clean.split())

        if not source_words or not target_words:
            return 0.0

        intersection = source_words & target_words
        union = source_words | target_words
        jaccard = len(intersection) / len(union)

        exact_matches = len(intersection)
        if exact_matches > 0:
            jaccard += 0.2 * exact_matches

        for source_word in source_words:
            for target_word in target_words:
                if source_word in target_word or target_word in source_word:
                    jaccard += 0.1

        return min(jaccard, 1.0)

    def _analyze_data_patterns(
        self, source_cols: List[str], sample_data: pd.DataFrame
    ) -> Dict[str, Dict[str, Any]]:
        """Step 2: Look at actual data to understand what each column contains"""
        insights = {}

        for col in source_cols:
            if col not in sample_data.columns:
                continue

            data_series = sample_data[col].dropna()
            if len(data_series) == 0:
                continue

            insights[col] = {
                "data_type": str(data_series.dtype),
                "value_pattern": self._detect_value_pattern(data_series),
                "business_domain": self._guess_business_domain(col, data_series),
            }

        return insights

    def _detect_value_pattern(self, series: pd.Series) -> str:
        """Detect specific patterns in the values"""
        if pd.api.types.is_numeric_dtype(series):
            min_val = series.min()
            max_val = series.max()

            if 50 <= min_val and max_val <= 500:
                return "typical_manhours"
            elif 0 <= min_val and max_val <= 5:
                return "typical_incidents"
            elif 80 <= min_val and max_val <= 100:
                return "high_percentage"

        return "general"

    def _guess_business_domain(self, col_name: str, data_series: pd.Series) -> str:
        """Guess which business domain this column belongs to"""
        col_lower = col_name.lower()

        for domain, info in self.operational_vocabulary.items():
            for keyword in info["keywords"]:
                if keyword in col_lower:
                    return domain

        return "general"

    def _apply_data_insights(
        self, semantic_mappings: Dict, data_insights: Dict, target_cols: List[str]
    ) -> Dict:
        """Step 3: Use data analysis to boost confidence of good mappings"""
        for source_col, insights in data_insights.items():
            data_type = insights["data_type"]
            business_domain = insights["business_domain"]

            for target_col in target_cols:
                current_score = semantic_mappings[source_col].get(target_col, 0)

                # Boost based on data type alignment
                if data_type == "percentage" and "%" in target_col:
                    semantic_mappings[source_col][target_col] = min(
                        current_score + 0.3, 1.0
                    )
                elif "manhour" in target_col.lower():
                    semantic_mappings[source_col][target_col] = min(
                        current_score + 0.3, 1.0
                    )
                elif "incident" in target_col.lower():
                    semantic_mappings[source_col][target_col] = min(
                        current_score + 0.3, 1.0
                    )

                # Boost based on business domain
                if business_domain in self.operational_vocabulary:
                    domain_targets = self.operational_vocabulary[business_domain][
                        "target_columns"
                    ]
                    if target_col in domain_targets:
                        semantic_mappings[source_col][target_col] = min(
                            current_score + 0.2, 1.0
                        )

        return semantic_mappings
